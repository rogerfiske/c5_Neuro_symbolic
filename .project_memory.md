# Project Memory - C5 Neuro-Symbolic Predictive Maintenance

> **Purpose**: Quick reference for key facts, decisions, and context across sessions
> **Updated**: 2026-01-28
> **Format**: Concise, searchable, assistant-friendly

---

## User Profile

**Name**: y
**Role**: Research project lead (NOT a trained programmer or data scientist)
**Technical Background**: Depends on AI agents for code implementation, debugging, technical execution
**Skill Level**: Domain expertise, research direction, strategic decisions
**Environment**: Windows 11, local PC + RunPod access (H200/B200)
**Preferences**: Reproducibility-focused, interpretability-driven, values evidence over complexity
**Working Style**: Guides research direction; agents implement technical details

---

## Project Identity

**Name**: C5 Neuro-Symbolic Predictive Maintenance
**Code**: c5_neuro_symbolic
**Type**: ML research project (neuro-symbolic AI)
**Domain**: Predictive maintenance for 5-machine production line
**Phase**: All research complete. Production ready.

**Git Repository**:
- **URL**: https://github.com/rogerfiske/c5_Neuro_symbolic.git
- **Branch**: main

---

## Core Problem

**Input**: Historical daily part shipments (11,685 records, 1992-2026)
**Output**: Next-day staged parts pool (size K=30)
**Constraint**: Exactly 5 unique parts needed per day (from pool of 39)
**Objective**: Maximize Good-or-better rate while minimizing K and maintaining stability

**Tier Definitions**:
- 5/5 in pool = Excellent
- 4/5 in pool = Good
- <=3/5 in pool = Unacceptable

---

## Current Best Performance (Hybrid Strategy @K=30)

| Metric | Value |
|--------|-------|
| Excellent (5/5) | 27.0% |
| Good (4/5) | 42.9% |
| **Good-or-Better** | **69.9%** |
| Unacceptable (<=3/5) | 30.1% |

**Approach**: Neural model for parts 1-11, 13-39 + frequency baseline for Part 12 only

### Performance Comparison (same test split)

| Strategy | GoB @K=30 |
|----------|-----------|
| **Hybrid (production)** | **69.9%** |
| Pure Neural | 68.2% |
| Pure Baseline | 65.8% |
| Best Ensemble | 67.1% |
| Best Column-Enhanced | 68.1% |

### Original vs Achieved Targets

| Metric | Original Target | Achieved | Notes |
|--------|----------------|----------|-------|
| GoB | >=90% | 69.9% | Near-uniform distribution creates hard ceiling |
| Unacceptable | <=5% | 30.1% | ~30% appears to be theoretical floor at K=30 |
| K | 20-27 | 30 | K=30 found optimal during research |

The near-uniform part distribution (CV ~2.4%) limits predictability. The 90% GoB target was aspirational; 69.9% appears near the practical ceiling.

---

## Key Decisions Made

### Production Strategy (2026-01-27)
- **Hybrid approach**: Neural for 38 parts, frequency baseline for Part 12
- **Part 12 anomaly**: Neural ranks it 32-35 (just outside K=30); baseline catches it 36.5%
- **Hybrid lift**: +1.6pp over pure neural (12 days improved, 0 degraded)

### Optimal Pool Size (2026-01-26)
- **K=30** found optimal (outside original 20-27 target)
- K=39 achieves 100% trivially (stock everything) -- not a model achievement
- Meaningful evaluation must be at fixed K

### Neural Architecture (2026-01-23)
- **Transformer** encoder (3 layers, 2 heads, 128-dim embeddings)
- **14-day sequence** (short context outperformed 30/45/60 days)
- Trained on RunPod H200 (50-trial Optuna hyperopt)

### Research Directions Closed
- **Column-enhanced neural** (2026-01-28): 6 architectures tested, none beat hybrid
- **Per-column frequency** (2026-01-28): Worse than global baseline (-1.78pp)
- **Ensemble strategies** (2026-01-26): 6 strategies tested, pure neural wins
- **Symbolic rules**: Minimal metric improvement (+0pp), interpretability only

---

## Key Findings

1. **K=39 = 100% trivially** -- Pool size expansion is inventory expansion, not model improvement
2. **Neural lift is ~4pp over baseline at fixed K** -- Meaningful but modest
3. **Neural excels on hard parts** -- +36.8pp lift on hard parts vs +2.8pp on easy parts
4. **Hard parts** (6): [8, 12, 13, 22, 23, 39] -- baseline <50% recall
5. **Part 12 is the outlier** -- Neural gives 0% recall; baseline gives 36.5% (hybrid fixes this)
6. **Short context wins** -- 14-day history outperforms longer sequences
7. **Baselines are hard to beat** -- Near-uniform distribution makes frequency surprisingly competitive
8. **Column position doesn't help** -- Neither neural nor frequency benefits from column info

---

## Completed Research Phases

### Phase 1: Baseline & Neural Comparison (2026-01-21 to 2026-01-26)
- All 8 Synapse workflows completed
- Baseline: 65.8% GoB, Neural: 68.2% GoB @K=30
- K=30 found optimal

### Phase 2: Per-Part & Ensemble Analysis (2026-01-26)
- Neural lift concentrated on hard parts (+36.8pp)
- 6 ensemble strategies tested; pure neural wins
- Part 12 anomaly identified

### Part 12 Investigation (2026-01-27)
- Root cause: Part 12 probability 0.004 below cutoff
- Hybrid strategy implemented: +1.6pp over pure neural

### Column-Enhanced Research (2026-01-28)
- 6 neural architectures with column info: all worse than hybrid
- Per-column frequency: worse than global baseline
- Research direction CLOSED

---

## Critical Files

### Dataset
- **Path**: `data/raw/CA5_date.csv`
- **Records**: 11,685
- **Date Range**: 1992-02-04 to 2026-01-21

### Production
- **Inference**: `scripts/production_inference.py`
- **Checkpoint**: `outputs/best_model/checkpoints/`
- **Usage**: `python scripts/production_inference.py [--date YYYY-MM-DD] [--baseline-only]`

### Scripts (all 8 workflows)
- `scripts/data_profile.py`
- `scripts/baseline_suite.py`
- `scripts/feature_schema.py`
- `scripts/rulebook_draft.py`
- `scripts/neural_prototype.py`
- `scripts/hybrid_inference.py`
- `scripts/k_optimizer.py`
- `scripts/ablation_report.py`

### RunPod Package
- `runpod_package/` -- Deep learning pipeline (models, training, hyperopt)

### Agent
- **Agent YAML**: `_bmad-output/bmb-creations/synapse/synapse.agent.yaml`
- **Sidecar**: `_bmad-output/bmb-creations/synapse/synapse-sidecar/`
- **Activation**: `/synapse`

---

## Synapse Agent Commands

| Code | Command | Purpose |
|------|---------|---------|
| DP | data-profile | Validate schema, gaps, distributions, drift |
| BL | baseline-suite | Build strong baselines with tier metrics |
| FS | feature-schema | Engineer features with leakage audits |
| RD | rulebook-draft | Discover symbolic rules with evidence |
| NP | neural-model-prototype | Build calibrated neural scorers |
| HI | hybrid-inference | Combine neural + symbolic reasoning |
| KO | k-optimizer | Choose optimal K under constraints |
| AR | ablation-report | Systematic comparison with conclusions |

---

## Computational Resources

### Local PC
- **CPU**: AMD Ryzen 9 6900HX
- **RAM**: 64GB
- **GPU**: AMD Radeon RX 6600M (8GB)
- **Used For**: Data profiling, baselines, K-sweep, ablation, inference

### RunPod H200 (Phase 1)
- 50-trial hyperparameter optimization, final model training
- ~1 hour total

### RunPod B200 (Phase 2 + Column Research)
- Per-part inference, ensemble experiments, column-enhanced training
- ~30 minutes total

---

## Communication Preferences

**User prefers**:
- Concise, structured responses
- Evidence-based recommendations
- Clear next steps / action items
- Numbered lists and checklists
- No time estimates

**Avoid**:
- Excessive praise or validation
- Emojis (unless requested)
- Vague or ambiguous guidance

---

## Possible Future Directions

1. **Operationalize**: Monitoring dashboard, automated retraining, drift detection
2. **Dynamic K**: Vary pool size daily based on model confidence
3. **Conformal prediction**: Statistical coverage guarantees
4. **Maintenance scheduling**: Regime detection and wear-mode inference (PRD Section 8, never pursued)
5. **Attention visualization**: Understand what patterns the model learned

---

## Session Log

| Date | Focus | Key Result |
|------|-------|------------|
| 2026-01-21 | Project initiation | Dataset obtained, PRD created |
| 2026-01-22 | Agent creation | Synapse agent built (8 workflows) |
| 2026-01-23 | RunPod deployment | Neural pipeline built, H200 hyperopt started |
| 2026-01-26 | Phase 1 & 2 complete | 69% GoB, neural +36.8pp on hard parts |
| 2026-01-27 | Part 12 investigation | Hybrid strategy: 69.9% GoB |
| 2026-01-28 | Column-enhanced research | No improvement, direction closed |

---

**Memory Last Updated**: 2026-01-28
