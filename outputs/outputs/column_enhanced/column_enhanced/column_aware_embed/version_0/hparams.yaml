batch_size: 64
dropout: 0.2
embed_dim: 128
embedding_type: column_aware
encoder_type: transformer
focal_gamma: 2.0
hidden_dim: 192
learning_rate: 0.0001
max_epochs: 50
name: column_aware_embed
num_heads: 2
num_layers: 3
num_parts: 39
pool_size: 30
sequence_length: 14
test_years: 2.0
use_column_attention: false
use_column_heads: false
val_years: 0.5
weight_decay: 0.01
